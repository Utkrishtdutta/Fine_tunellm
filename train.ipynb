{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utkrisht.Dutta\\Downloads\\Fine_tunellm\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Utkrisht.Dutta\\Downloads\\Fine_tunellm\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DataFrame with QA pairs\n",
    "df = pd.read_excel(\"training_data_assignment.xlsx\")  # Replace with your dataset path\n",
    "df.columns = ['question', 'answer']\n",
    "\n",
    "# Combine questions and answers into a single string\n",
    "df['input_text'] = df['question'] + \" [SEP] \" + df['answer']\n",
    "texts = df['input_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize your dataset using the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Save the texts to a text file with the correct encoding (e.g., 'utf-8')\n",
    "with open(\"training_data_assignment.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for text in texts:\n",
    "        file.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utkrisht.Dutta\\Downloads\\Fine_tunellm\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a TextDataset\n",
    "text_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"training_data_assignment.txt\",  # Specify the path to the dataset file\n",
    "    block_size=128,  # Adjust the block size based on your data\n",
    ")\n",
    "\n",
    "# Fine-tune the GPT-2 model on your QA dataset\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 500/510 [27:31<00:27,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0923, 'learning_rate': 9.80392156862745e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 510/510 [27:57<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1677.5683, 'train_samples_per_second': 0.607, 'train_steps_per_second': 0.304, 'train_loss': 3.082860273473403, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=510, training_loss=3.082860273473403, metrics={'train_runtime': 1677.5683, 'train_samples_per_second': 0.607, 'train_steps_per_second': 0.304, 'train_loss': 3.082860273473403, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-gpt2-qa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,  # Adjust as needed\n",
    "    per_device_train_batch_size=2,  # Adjust based on your GPU memory\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=text_dataset,  # Use the TextDataset for training\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine-tuned-gpt2-qa\\\\tokenizer_config.json',\n",
       " 'fine-tuned-gpt2-qa\\\\special_tokens_map.json',\n",
       " 'fine-tuned-gpt2-qa\\\\vocab.json',\n",
       " 'fine-tuned-gpt2-qa\\\\merges.txt',\n",
       " 'fine-tuned-gpt2-qa\\\\added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned GPT-2 model and tokenizer\n",
    "model.save_pretrained(\"fine-tuned-gpt2-qa\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-gpt2-qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\Utkrisht.Dutta\\Downloads\\Fine_tunellm\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the stocks? [SEP] The stock market is a market where investors buy and sell\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the stocks?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
